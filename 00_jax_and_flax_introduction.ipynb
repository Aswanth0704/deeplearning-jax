{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why learn jax:</br>\n",
    "It is extremely fast\n",
    "How is jax faster than pytorch?</br>\n",
    "Because jax uses just in time compilation compared to pytorchs dynamic graph construction.\n",
    "\n",
    "1. Jax functions should not have side effects ==> they are not allowed to affect any variable outside of their namespaces.\n",
    "2. Jax compiles functions based on anticipated shapes of all arrays/tensors in the function. This can be problematic when shapes or progrmam flow within the function depends on the values of the tensors (e.g y = x[x>=2]. Here the shape of y is determined by how many values in x are greater than 2.)\n",
    "\n",
    "More resources:\n",
    "1. Jax 101 \n",
    "2. Jax the sharp bits.\n",
    "3. Jax for impatient\n",
    "4. Flax basics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9d/yx8gqbb90hbg0j25szlgylq00000gp/T/ipykernel_68335/2920860919.py:11: DeprecationWarning: `set_matplotlib_formats` is deprecated since IPython 7.23, directly use `matplotlib_inline.backend_inline.set_matplotlib_formats()`\n",
      "  set_matplotlib_formats('svg', 'pdf') # for exports\n"
     ]
    }
   ],
   "source": [
    "# import standard libraris\n",
    "import os\n",
    "import math\n",
    "import numpy as np  \n",
    "import time\n",
    "\n",
    "# imports for plotting.\n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline\n",
    "from IPython.display import set_matplotlib_formats\n",
    "set_matplotlib_formats('svg', 'pdf') # for exports\n",
    "from matplotlib.colors import to_rgba\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "## progress bar\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jax as NumPy on accelerators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using jax version 0.4.35\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "print('Using jax version', jax.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "a = jnp.zeros((2, 3), dtype= jnp.float32)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 5]\n"
     ]
    }
   ],
   "source": [
    "b = jnp.arange(6)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "jaxlib.xla_extension.ArrayImpl"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.__class__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike numpy arrays, jax arrays can execute the same code on different backends like cpus, gpus or tpus. Hence, device arays represent an array on one of the backends. we can check the device of an array using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CpuDevice(id=0)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It would have been on gpu if there is one! But inorder to make the array on cpu we can use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "b_cpu = jax.device_get(b) \n",
    "print(b_cpu.__class__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To explicitly push the Numpy array to GPU use jax.device_put()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'jaxlib.xla_extension.ArrayImpl'>\n"
     ]
    }
   ],
   "source": [
    "b_gpu = jax.device_put(b_cpu)\n",
    "print(b_gpu.__class__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jax handles the device clash itself. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([ 0,  2,  4,  6,  8, 10], dtype=int32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_cpu + b_gpu "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print all the available devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[CpuDevice(id=0)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jax.devices()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Immutable tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numpy arrays are mutable meaning the value of array elemets can be changed in place. example: b[0] = 2. Device arrays are immutable. No inplace operations are possible (jax requires programs to be pure functions). We use b.at[0].set(1) which returns a new array  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original array [0 1 2 3 4 5]\n",
      "new array [1 1 2 3 4 5]\n"
     ]
    }
   ],
   "source": [
    "b_new = b.at[0].set(1)\n",
    "print('original array', b)\n",
    "print('new array', b_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pseudo Random numbers in Jax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid automatic change of seed outside the namespace, in jax we need to pass the seed to generate a random number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = jax.random.PRNGKey(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jax_random_number_1 -0.18471177\n",
      "jax_random_number_2 -0.18471177\n"
     ]
    }
   ],
   "source": [
    "## not an ideal way of generating random numbers\n",
    "jax_random_number_1 = jax.random.normal(rng)\n",
    "jax_random_number_2 = jax.random.normal(rng)\n",
    "print('jax_random_number_1', jax_random_number_1)\n",
    "print('jax_random_number_2', jax_random_number_2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That produced same random number because of PRNGKey. One can always get new keys by splitting the old one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy_random_number_1:  0.4967141530112327\n",
      "numpy_random_number_2:  -0.13826430117118466\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42) # set the seed\n",
    "np_random_number_1 = np.random.normal()\n",
    "np_random_number_2 = np.random.normal()\n",
    "\n",
    "print('numpy_random_number_1: ',np_random_number_1) \n",
    "print('numpy_random_number_2: ',np_random_number_2) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two different numbers since the key (state of the random number generator) changes. Not allowed in jax --> side-effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jax_random_number_1  0.107961535\n",
      "jax_random_number_2  -1.2226542\n"
     ]
    }
   ],
   "source": [
    "rng, subkey1, subkey2 = jax.random.split(rng, num= 3)\n",
    "jax_random_number_1 = jax.random.normal(subkey1)\n",
    "jax_random_number_2 = jax.random.normal(subkey2)\n",
    "print(\"jax_random_number_1 \", jax_random_number_1)\n",
    "print(\"jax_random_number_2 \", jax_random_number_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function transformations with Jaxpr.\n",
    "1. Always write the code in the form of pure functions => no side effects.\n",
    "2. Jaxpr: Jax transforms a given function to a small and well behaved intermediate form. We can check which operations are performed on which array and what shapes the arrays are.\n",
    "3. Then based on the the jaxpr, jax then interprets the function with transformation-specific interpretation rules: automatic differentiation or compiling a function in XLA to efficiently use the accelerator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets use the below function to understand the jaxpr transformation.\n",
    "$$y = 1/|x| \\sum_i \\left[(x_i + 2)^2 +3\\right]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:  [0. 1. 2.]\n",
      "output:  12.666667\n"
     ]
    }
   ],
   "source": [
    "def simple_graph(x):\n",
    "    x = x + 2\n",
    "    x = x**2\n",
    "    x = x + 3\n",
    "    y = x.mean()\n",
    "    return y\n",
    "\n",
    "inp = jnp.arange(3, dtype= jnp.float32)\n",
    "print(\"input: \", inp)\n",
    "print(\"output: \", simple_graph(inp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{ lambda ; a:f32[3]. let\n",
       "    b:f32[3] = add a 2.0\n",
       "    c:f32[3] = integer_pow[y=2] b\n",
       "    d:f32[3] = add c 3.0\n",
       "    e:f32[] = reduce_sum[axes=(0,)] d\n",
       "    f:f32[] = div e 3.0\n",
       "  in (f,) }"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jax.make_jaxpr(simple_graph)(inp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets look at the jaxpr representation of a function with side-effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{ lambda ; a:f32[3]. let\n",
       "    b:f32[3] = integer_pow[y=2] a\n",
       "    c:f32[] = reduce_sum[axes=(0,)] b\n",
       "    d:f32[] = sqrt c\n",
       "  in (d,) }"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "global_list = []\n",
    "\n",
    "def norm(x):\n",
    "    global_list.append(x) # making changes to a global variable --> not a pure function.\n",
    "    x = x**2\n",
    "    n = x.sum()\n",
    "    n = jnp.sqrt(n)\n",
    "    return n\n",
    "\n",
    "jax.make_jaxpr(norm)(inp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jaxpr ignored the operation with side effects!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jax takes a function and gives us another function which directly computes the gradients for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradients [1.3333334 2.        2.6666667]\n"
     ]
    }
   ],
   "source": [
    "# jax.grad\n",
    "grad_function = jax.grad(simple_graph)\n",
    "gradients = grad_function(inp)\n",
    "print('Gradients', gradients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{ lambda ; a:f32[3]. let\n",
       "    b:f32[3] = add a 2.0\n",
       "    c:f32[3] = integer_pow[y=2] b\n",
       "    d:f32[3] = integer_pow[y=1] b\n",
       "    e:f32[3] = mul 2.0 d\n",
       "    f:f32[3] = add c 3.0\n",
       "    g:f32[] = reduce_sum[axes=(0,)] f\n",
       "    _:f32[] = div g 3.0\n",
       "    h:f32[] = div 1.0 3.0\n",
       "    i:f32[3] = broadcast_in_dim[broadcast_dimensions=() shape=(3,)] h\n",
       "    j:f32[3] = mul i e\n",
       "  in (j,) }"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jax.make_jaxpr(grad_function)(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Array(12.666667, dtype=float32),\n",
       " Array([1.3333334, 2.       , 2.6666667], dtype=float32))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# both value and gradient of a function for an input.\n",
    "val_grad_funct = jax.value_and_grad(simple_graph)\n",
    "val_grad_funct(inp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytrees help us in dealing with nested structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 'a', <object object at 0x117e4ba30>]      has length 3 leaves: [1, 'a', <object object at 0x117e4ba30>]\n",
      "(1, (2, 3), ())                               has length 3 leaves: [1, 2, 3]\n",
      "[1, {'k1': 2, 'k2': (3, 4)}, 5]               has length 5 leaves: [1, 2, 3, 4, 5]\n",
      "{'a': 2, 'b': (2, 3)}                         has length 3 leaves: [2, 2, 3]\n",
      "Array([1, 2, 3], dtype=int32)                 has length 1 leaves: [Array([1, 2, 3], dtype=int32)]\n"
     ]
    }
   ],
   "source": [
    "# example:\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "example_tree = [\n",
    "    [1, 'a', object()],\n",
    "    (1, (2, 3), ()),\n",
    "    [1, {'k1':2, 'k2': (3, 4)}, 5],\n",
    "    {'a': 2, 'b': (2, 3)},\n",
    "    jnp.array([1, 2, 3]) # itself is a leaf\n",
    "]\n",
    "\n",
    "# print how many leaves the pytrees have\n",
    "for pytree in example_tree:\n",
    "    # this 'jax.tree.leaves()' method extracts the flattened leaves from pytree\n",
    "    leaves = jax.tree.leaves(pytree)\n",
    "    print(f'{repr(pytree): <45} has length {len(leaves)} leaves: {leaves}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. lists, tuples, and dicts are considered as pytree.\n",
    "2. Any object that is not in the pytree registry is treated as leaf.\n",
    "3. pytree registry can be extended to user defined contained  classes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common Pytree functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. jax.tree.map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2, 4, 6], [2, 4], [2, 4, 6, 8]]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_of_lists = [\n",
    "    [1, 2, 3],\n",
    "    [1,2],\n",
    "    [1, 2, 3, 4]\n",
    "]\n",
    "\n",
    "jax.tree.map(lambda x: x*2, list_of_lists)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jax.tree.map also allows mapping a N-ary function over multiple arguments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2, 4, 6], [2, 4], [2, 4, 6, 8]]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "another_list_of_lists  = list_of_lists\n",
    "jax.tree.map(lambda x, y: x + y, list_of_lists, another_list_of_lists)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using the multiple arguments with jax.tree.map, the structure of the inputs must match. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## jax.tree.map with ML model parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pytrees can be useful in training a MLP.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def init_mlp_params(layer_widths):\n",
    "    params = []\n",
    "    for n_in, n_out in zip(layer_widths[:-1], layer_widths[1:]):\n",
    "        params.append(\n",
    "            dict(\n",
    "                weights = np.random.normal(size= (n_in, n_out))*np.sqrt(2/n_in),\n",
    "                biases = np.ones(shape= (n_out,))\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return params\n",
    "\n",
    "params = init_mlp_params([1, 128, 128, 1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'biases': (128,), 'weights': (1, 128)},\n",
       " {'biases': (128,), 'weights': (128, 128)},\n",
       " {'biases': (1,), 'weights': (128, 1)}]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use jax.tree.map to check the shapes of the inital parameters\n",
    "\n",
    "jax.tree.map(lambda x: x.shape, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function for training the mlp model:\n",
    "# define forward pass\n",
    "def forward(params, x):\n",
    "    *hidden, last = params\n",
    "    for layer in hidden:\n",
    "        x = jax.nn.relu(x@layer['weights'] + layer['biases'])\n",
    "        return x@last['weights'] +last['biases']\n",
    "\n",
    "# define the loss function:\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jaxenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
